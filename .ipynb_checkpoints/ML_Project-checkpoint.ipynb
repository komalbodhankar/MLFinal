{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import json\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproceesing(df):\n",
    "\n",
    "    def group_edu(x):\n",
    "            if x <= 5:\n",
    "                return \"<6\"\n",
    "            elif x >= 13:\n",
    "                return \">12\"\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "    def age_cut(x):\n",
    "        if x >= 70:\n",
    "            return \">=70\"\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def group_race(x):\n",
    "        if x == \"White\":\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    # Cluster education and age attributes.\n",
    "    # Limit education range\n",
    "    df[\"education-num\"] = df[\"education-num\"].apply(lambda x: group_edu(x))\n",
    "    df[\"education-num\"] = df[\"education-num\"].astype(\"category\")\n",
    "\n",
    "    # Limit age range\n",
    "    df[\"age\"] = df[\"age\"].astype(int)\n",
    "    df[\"age\"] = df[\"age\"].apply(lambda x: x // 10 * 10)\n",
    "    df[\"age\"] = df[\"age\"].apply(lambda x: age_cut(x))\n",
    "\n",
    "    # Group race\n",
    "    df[\"race\"] = df[\"race\"].apply(lambda x: group_race(x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproceesing_v2(train,test,\n",
    "    protected_attribute_name=\"\",\n",
    "    privileged_classes=[],\n",
    "    missing_value=[],\n",
    "    features_to_drop=[],\n",
    "    categorical_features=[],\n",
    "    favorable_classes=[],\n",
    "    normalize=True,\n",
    "):\n",
    "    cols = [\n",
    "        x\n",
    "        for x in train.columns\n",
    "        if x\n",
    "        not in (\n",
    "            features_to_drop\n",
    "            + [protected_attribute_name]\n",
    "            + categorical_features\n",
    "            + [\"result\"]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    result = []\n",
    "    for df in [train, test]:\n",
    "        # drop useless features\n",
    "        df = df.drop(columns=features_to_drop)\n",
    "\n",
    "        # create one-hot encoding of categorical features\n",
    "        df = pd.get_dummies(df, columns=categorical_features, prefix_sep=\"=\")\n",
    "\n",
    "        # map protected attributes to privileged or unprivileged\n",
    "        pos = np.logical_or.reduce(\n",
    "            np.equal.outer(privileged_classes, df[protected_attribute_name].values)\n",
    "        )\n",
    "        df.loc[pos, protected_attribute_name] = 1\n",
    "        df.loc[~pos, protected_attribute_name] = 0\n",
    "        df[protected_attribute_name] = df[protected_attribute_name].astype(int)\n",
    "\n",
    "        # set binary labels\n",
    "        pos = np.logical_or.reduce(\n",
    "            np.equal.outer(favorable_classes, df[\"result\"].values)\n",
    "        )\n",
    "        df.loc[pos, \"result\"] = 1\n",
    "        df.loc[~pos, \"result\"] = 0\n",
    "        df[\"result\"] = df[\"result\"].astype(int)\n",
    "\n",
    "        result.append(df)\n",
    "\n",
    "    # standardize numeric columns\n",
    "    for col in cols:\n",
    "        data = result[0][col].tolist()\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        result[0][col] = (result[0][col] - mean) / std\n",
    "        result[1][col] = (result[1][col] - mean) / std\n",
    "\n",
    "    train = result[0]\n",
    "    test = result[1]\n",
    "    for col in train.columns:\n",
    "        if col not in test.columns:\n",
    "            test[col] = 0\n",
    "    cols = train.columns\n",
    "    test = test[cols]\n",
    "    assert all(\n",
    "        train.columns[i] == test.columns[i] for i in range(len(train.columns))\n",
    "    )\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_data_analysis(self, df_old, y=None, log=True):\n",
    "        df = df_old.copy()\n",
    "        if y is not None:\n",
    "            df[\"y hat\"] = (y > 0.5).astype(int)\n",
    "        s = self.protected_attribute_name\n",
    "        res = dict()\n",
    "        n = df.shape[0]\n",
    "        y1 = df.loc[df[\"result\"] == 1].shape[0] / n\n",
    "        if \"y hat\" in df.columns:\n",
    "            yh1s0 = (\n",
    "                df.loc[(df[s] == 0) & (df[\"y hat\"] == 1)].shape[0]\n",
    "                / df.loc[df[s] == 0].shape[0]\n",
    "            )\n",
    "            yh1s1 = (\n",
    "                df.loc[(df[s] == 1) & (df[\"y hat\"] == 1)].shape[0]\n",
    "                / df.loc[df[s] == 1].shape[0]\n",
    "            )\n",
    "            yh1y1s0 = (\n",
    "                df.loc[(df[\"y hat\"] == 1) & (df[\"result\"] == 1) & (df[s] == 0)].shape[0]\n",
    "                / df.loc[(df[\"result\"] == 1) & (df[s] == 0)].shape[0]\n",
    "            )\n",
    "            yh1y1s1 = (\n",
    "                df.loc[(df[\"y hat\"] == 1) & (df[\"result\"] == 1) & (df[s] == 1)].shape[0]\n",
    "                / df.loc[(df[\"result\"] == 1) & (df[s] == 1)].shape[0]\n",
    "            )\n",
    "            yh0y0s0 = (\n",
    "                df.loc[(df[\"y hat\"] == 0) & (df[\"result\"] == 0) & (df[s] == 0)].shape[0]\n",
    "                / df.loc[(df[\"result\"] == 0) & (df[s] == 0)].shape[0]\n",
    "            )\n",
    "            yh0y0s1 = (\n",
    "                df.loc[(df[\"y hat\"] == 0) & (df[\"result\"] == 0) & (df[s] == 1)].shape[0]\n",
    "                / df.loc[(df[\"result\"] == 0) & (df[s] == 1)].shape[0]\n",
    "            )\n",
    "\n",
    "            res[\"acc\"] = df.loc[df[\"result\"] == df[\"y hat\"]].shape[0] / n\n",
    "\n",
    "            res[\"DP\"] = np.abs(yh1s1 - yh1s0)\n",
    "            tpr = yh1y1s0 - yh1y1s1\n",
    "            fpr = yh0y0s0 - yh0y0s1\n",
    "            res[\"EO\"] = np.abs(tpr) * y1 + np.abs(fpr) * (1 - y1)\n",
    "\n",
    "            fair_variables = self.fair_variables\n",
    "            count = (\n",
    "                df.groupby(fair_variables + [s])\n",
    "                .count()[\"y hat\"]\n",
    "                .reset_index()\n",
    "                .rename(columns={\"y hat\": \"count\"})\n",
    "            )\n",
    "            count_y = (\n",
    "                df.groupby(fair_variables + [s])\n",
    "                .sum()[\"y hat\"]\n",
    "                .reset_index()\n",
    "                .rename(columns={\"y hat\": \"count_y\"})\n",
    "            )\n",
    "            count_merge = pd.merge(count, count_y, how=\"outer\", on=fair_variables + [s])\n",
    "            count_merge[\"ratio\"] = count_merge[\"count_y\"] / count_merge[\"count\"]\n",
    "            count_merge = count_merge.drop(columns=[\"count\", \"count_y\"])\n",
    "            count_merge[\"ratio\"] = (2 * count_merge[s] - 1) * count_merge[\"ratio\"]\n",
    "            if len(self.fair_variables) > 0:\n",
    "                result = (\n",
    "                    count_merge.groupby(fair_variables)\n",
    "                    .sum()[\"ratio\"]\n",
    "                    .reset_index(drop=True)\n",
    "                    .values\n",
    "                )\n",
    "            else:\n",
    "                result = count_merge.sum()[\"ratio\"]\n",
    "\n",
    "        if len(self.fair_variables) > 0:\n",
    "            fairs = (\n",
    "                df.groupby(self.fair_variables).count()[s].reset_index(drop=True).values\n",
    "            )\n",
    "            fairs = fairs / np.sum(fairs)\n",
    "        else:\n",
    "            fairs = 1\n",
    "        res[\"CF\"] = np.sum(np.abs(result) * fairs)\n",
    "\n",
    "        if log:\n",
    "            for key, value in res.items():\n",
    "                print(key, \"=\", value)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "                \"age\",\n",
    "                \"workclass\",\n",
    "                \"fnlwgt\",\n",
    "                \"education\",\n",
    "                \"education-num\",\n",
    "                \"marital-status\",\n",
    "                \"occupation\",\n",
    "                \"relationship\",\n",
    "                \"race\",\n",
    "                \"sex\",\n",
    "                \"capital-gain\",\n",
    "                \"capital-loss\",\n",
    "                \"hours-per-week\",\n",
    "                \"native-country\",\n",
    "                \"result\",\n",
    "            ]\n",
    "\n",
    "dataframe_train = pd.read_csv('./adult.data', names=column_names)\n",
    "dataframe_test = pd.read_csv('./adult.test', names=column_names)\n",
    "# dataframe_test\n",
    "# dataframe_train\n",
    "\n",
    "dataframe_test.isin(['?']).sum(axis=0)\n",
    "dataframe_test['native-country'] = dataframe_test['native-country'].replace('?', np.nan)\n",
    "dataframe_test['workclass'] = dataframe_test['workclass'].replace('?', np.nan)\n",
    "dataframe_test['occupation'] = dataframe_test['occupation'].replace('?', np.nan)\n",
    "dataframe_test.dropna(how='any', inplace=True)\n",
    "\n",
    "preproceesing(dataframe_train)\n",
    "preproceesing(dataframe_test)\n",
    "# dataframe_train\n",
    "\n",
    "categorical_features = [\n",
    "                \"workclass\",\n",
    "                \"education\",\n",
    "                \"age\",\n",
    "                \"race\",\n",
    "                \"education-num\",\n",
    "                \"marital-status\",\n",
    "                \"occupation\",\n",
    "                \"relationship\",\n",
    "                \"native-country\",\n",
    "            ]\n",
    "\n",
    "protected_attribute_name = \"sex\"\n",
    "privileged_classes = [\"Male\"]\n",
    "missing_value=[\"?\"]\n",
    "features_to_drop=[\"fnlwgt\"]\n",
    "categorical_features=categorical_features\n",
    "favorable_classes=[\">50K\", \">50K.\"]\n",
    "col = dataframe_train.columns.values\n",
    "fair_variables = [ele for ele in col if \"occupation\" in ele]\n",
    "\n",
    "train,test=preproceesing_v2(dataframe_train,dataframe_test,\n",
    "                protected_attribute_name,privileged_classes,\n",
    "                missing_value,features_to_drop,categorical_features,\n",
    "                favorable_classes)\n",
    "\n",
    "\n",
    "display(train)\n",
    "display(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairVariableUsage(nn.Module):\n",
    "    def __init__(self, model, config):\n",
    "        super(FairVariableUsage, self).__init__()\n",
    "        dataset = \"adult\"\n",
    "        task = config[\"task\"]\n",
    "        self.fair_coeff = config[\"fair_coeff\"]\n",
    "        self.task = task\n",
    "        self.name = f\"{model}_{task}_{dataset}_fair_coeff_{self.fair_coeff}\"\n",
    "\n",
    "    def predicted_loss(self, x, y, w):\n",
    "        return 0\n",
    "\n",
    "    def audit_loss(self, x, s, f, w):\n",
    "        return 0\n",
    "\n",
    "    def loss(self, x, y, s, f, w_pred, w_audit):\n",
    "        loss = self.predicted_loss(x, y, w_pred) - self.fair_coeff * self.audit_loss(\n",
    "            x, s, f, w_audit\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def predicted_weight(self, df):\n",
    "        n = df.shape[0]\n",
    "        return torch.ones((n, 1)) / n\n",
    "\n",
    "    def audit_weight(self, df, s, f):\n",
    "        return torch.tensor([1.0 / df.shape[0]] * df.shape[0])\n",
    "\n",
    "    def y_fwd(self, x):\n",
    "        pass\n",
    "\n",
    "    def fwd(self, x):\n",
    "        self.y_fwd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, shapes, acti):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.acti = acti\n",
    "        self.fc = nn.ModuleList()\n",
    "        for i in range(0, len(shapes) - 1):\n",
    "            self.fc.append(nn.Linear(shapes[i], shapes[i + 1]))\n",
    "\n",
    "    def ff_fwd(self, x):\n",
    "        for i, fc in enumerate(self.fc):\n",
    "            x = fc(x)\n",
    "            if i == len(self.fc) - 1:\n",
    "                break\n",
    "            if self.acti == \"relu\":\n",
    "                x = F.relu(x)\n",
    "            elif self.acti == \"sigmoid\":\n",
    "                x = F.sigmoid(x)\n",
    "            elif self.acti == \"softplus\":\n",
    "                x = F.softplus(x)\n",
    "            elif self.acti == \"leakyrelu\":\n",
    "                x = F.leaky_relu(x)\n",
    "        return x\n",
    "\n",
    "    def freeze(self):\n",
    "        for para in self.parameters():\n",
    "            para.requires_grad = False\n",
    "\n",
    "    def activate(self):\n",
    "        for para in self.parameters():\n",
    "            para.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(FairVariableUsage):\n",
    "    def __init__(self, config, n_fair):\n",
    "        super(Model, self).__init__(\"DCFR\", config)\n",
    "\n",
    "        self.encoder = FeedForward(\n",
    "            [config[\"xdim\"]] + config[\"encoder\"] + [config[\"zdim\"]], \"relu\"\n",
    "        )\n",
    "        self.prediction = FeedForward(\n",
    "            [config[\"zdim\"]] + config[\"prediction\"] + [config[\"ydim\"]],\n",
    "            \"relu\",\n",
    "        )\n",
    "\n",
    "        self.audit = FeedForward(\n",
    "            [config[\"zdim\"] + n_fair] + config[\"audit\"] + [config[\"sdim\"]],\n",
    "            \"relu\",\n",
    "        )\n",
    "\n",
    "    def model_y_fwd(self, x):\n",
    "        z = self.model_z_fwd(x)\n",
    "        y = self.prediction(z)\n",
    "        y = torch.sigmoid(y)\n",
    "        return y\n",
    "\n",
    "    def model_s_fwd(self, x, f):\n",
    "        z = self.model_z_fwd(x)\n",
    "        s = self.audit(torch.cat([z, f], dim=1))\n",
    "        s = torch.sigmoid(s)\n",
    "        return s\n",
    "\n",
    "    def model_z_fwd(self, x):\n",
    "        z = torch.nn.functional.relu(self.encoder(x))\n",
    "        return z\n",
    "\n",
    "    def fwd(self, x):\n",
    "        self.model_y_fwd(x)\n",
    "\n",
    "    def predicted_loss(self, x, y, w):\n",
    "        y_pred = self.model_y_fwd(x)\n",
    "        loss = self.weighted_cross_entropy(w, y, y_pred)\n",
    "        return loss\n",
    "\n",
    "    def loss_audit(self, x, s, f, w):\n",
    "        s_pred = self.model_s_fwd(x, f)\n",
    "        loss = self.weighted_mse(w, s, s_pred)\n",
    "        return loss\n",
    "\n",
    "    def weight_audit(self, df_old, s, f):\n",
    "        df = df_old.copy()\n",
    "        df[\"w\"] = 0.0\n",
    "\n",
    "        if self.task == \"DP\":\n",
    "            df[\"n_f\"] = df.shape[0]\n",
    "        else:\n",
    "            res = df.groupby(f).count()[\"w\"].reset_index().rename(columns={\"w\": \"n_f\"})\n",
    "            df = df.merge(res, on=f, how=\"left\")\n",
    "\n",
    "        res = (\n",
    "            df.groupby(f + [s])\n",
    "            .count()[\"w\"]\n",
    "            .reset_index()\n",
    "            .rename(columns={\"w\": \"n_s_f\"})\n",
    "        )\n",
    "        df = df.merge(res, on=f + [s], how=\"left\")\n",
    "\n",
    "        df[\"w\"] = 1 - df[\"n_s_f\"] / df[\"n_f\"]\n",
    "\n",
    "        res = torch.from_numpy(df[\"w\"].values).view(-1, 1)\n",
    "        res = res / res.sum()\n",
    "        return res\n",
    "\n",
    "    def predict_only(self):\n",
    "        self.audit.freeze()\n",
    "        self.prediction.activate()\n",
    "        self.encoder.activate()\n",
    "\n",
    "    def audit_only(self):\n",
    "        self.audit.activate()\n",
    "        self.prediction.freeze()\n",
    "        self.encoder.freeze()\n",
    "\n",
    "    def finetune_only(self):\n",
    "        self.audit.freeze()\n",
    "        self.prediction.activate()\n",
    "        self.encoder.freeze()\n",
    "\n",
    "    def predict_params(self):\n",
    "        return list(self.prediction.parameters()) + list(self.encoder.parameters())\n",
    "\n",
    "    def audit_params(self):\n",
    "        return self.audit.parameters()\n",
    "\n",
    "    def finetune_params(self):\n",
    "        return self.prediction.parameters()\n",
    "    \n",
    "    def weighted_mse(w, y, y_pred):\n",
    "        return torch.sum(w * (y - y_pred) * (y - y_pred))\n",
    "    \n",
    "    def weighted_cross_entropy(w, y, y_pred, eps=1e-8):\n",
    "        res = -torch.sum(w * (y * torch.log(y_pred + eps) + (1 - y) * torch.log(1 - y_pred + eps)))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Trainer:\n",
    "    def __init__(self, dataset,model,config):\n",
    "        self.dataset = \"adult\"\n",
    "        self.model = \"DCFR\"\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.epoch = config[\"epoch\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.optim = config[\"optim\"]\n",
    "        self.aud_steps = config[\"aud_steps\"]\n",
    "        self.seed = config[\"seed\"]\n",
    "        self.tensorboard = config[\"tensorboard\"]\n",
    "        \n",
    "        self.name = f\"{self.model.name}_{self.optim}_batch_size_{self.batch_size}_epoch_{self.epoch}_lr_{self.lr}_aud_steps_{self.aud_steps}_seed_{self.seed}\"\n",
    "        \n",
    "        self.output_name = (\n",
    "            \"{} (dataset: {}, task: {}, seed: {}, fair coeff: {})\".format(\n",
    "                config[\"model\"],\n",
    "                config[\"dataset\"],\n",
    "                config[\"task\"],\n",
    "                config[\"seed\"],\n",
    "                config[\"fair_coeff\"],\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.result_dir = (\"./results/\" + self.dataset.name + \"/\" + self.name)\n",
    "        \n",
    "        self.model_dir = (\"./model/\" + self.dataset.name + \"/\" + self.name)\n",
    "        \n",
    "        self.train_dataloader, self.val_dataloader = self._convert_to_dataloader(\n",
    "            dataset, self.batch_size\n",
    "        )\n",
    "        \n",
    "    def save(self, filename):\n",
    "        torch.save(self.model.state_dict(), self.model_dir + \"/\" + f\"{filename}.pth\")\n",
    "\n",
    "    def load(self, filename=\"best\"):\n",
    "        self.model.load_state_dict(torch.load(self.model_dir + \"/\" + f\"{filename}.pth\"))\n",
    "\n",
    "    def _convert_to_dataloader(self, dataset, batch_size):\n",
    "        for i, df in enumerate([dataset.train, dataset.val]):\n",
    "            x_idx = df.columns.values.tolist()\n",
    "            x_idx.remove(\"result\")\n",
    "            x = torch.from_numpy(df[x_idx].values).type(torch.float)\n",
    "            y = torch.from_numpy(df[\"result\"].values).view(-1, 1).type(torch.float)\n",
    "            s = (\n",
    "                torch.from_numpy(df[dataset.protected_attribute_name].values)\n",
    "                .view(-1, 1)\n",
    "                .type(torch.float)\n",
    "            )\n",
    "            f = torch.from_numpy(df[dataset.fair_variables].values).type(torch.float)\n",
    "            w_pred = self.model.weight_pred(df)\n",
    "            w_audit = self.model.weight_audit(\n",
    "                df, dataset.protected_attribute_name, dataset.fair_variables\n",
    "            ).view(-1, 1)\n",
    "            data = TensorDataset(x, y, s, f, w_pred, w_audit)\n",
    "            sampler = RandomSampler(data)\n",
    "            if i == 0:\n",
    "                self.n_train = x.shape[0]\n",
    "                train_dataloader = DataLoader(\n",
    "                    data, sampler=sampler, batch_size=batch_size\n",
    "                )\n",
    "            else:\n",
    "                self.n_val = x.shape[0]\n",
    "                val_dataloader = DataLoader(\n",
    "                    data, sampler=sampler, batch_size=batch_size\n",
    "                )\n",
    "        return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "        name = self.output_name\n",
    "        print(\n",
    "            \"=============================================================================\"\n",
    "        )\n",
    "        print(f\"Training for {name}\")\n",
    "        print(\n",
    "            \"=============================================================================\"\n",
    "        )\n",
    "\n",
    "        if os.path.exists(self.res_dir + \"/\" + \"train_done.txt\"):\n",
    "            print(\"Model has been trained!!\")\n",
    "            return\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = self.model.to(device)\n",
    "\n",
    "        pred_optimizer = getattr(optim, self.optim)(model.predict_params(), lr=self.lr)\n",
    "        if not model.audit_params() is None:\n",
    "            audit_optimizer = getattr(optim, self.optim)(\n",
    "                model.audit_params(), lr=self.lr\n",
    "            )\n",
    "\n",
    "        if self.tensorboard:\n",
    "            summary_writer = SummaryWriter(self.tensorboard_dir)\n",
    "\n",
    "        for epoch_i in range(self.epoch):\n",
    "\n",
    "            losses = {\n",
    "                \"prediction\": [],\n",
    "                \"audit\": [],\n",
    "                \"total\": [],\n",
    "            }\n",
    "\n",
    "            for step, batch in enumerate(self.train_dataloader):\n",
    "                x = batch[0].to(device)\n",
    "                y = batch[1].to(device)\n",
    "                s = batch[2].to(device)\n",
    "                f = batch[3].to(device)\n",
    "                w_pred = batch[4].to(device)\n",
    "                w_audit = batch[5].to(device)\n",
    "\n",
    "                # ***********Predict***********\n",
    "                model.predict_only()\n",
    "                loss = model.loss(x, y, s, f, w_pred, w_audit)\n",
    "                pred_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                pred_optimizer.step()\n",
    "\n",
    "                # ***********Audit***********\n",
    "                if not model.audit_params() is None:\n",
    "                    model.audit_only()\n",
    "                    for _ in range(self.aud_steps):\n",
    "                        loss = -model.loss(x, y, s, f, w_pred, w_audit)\n",
    "                        audit_optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        audit_optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    predicted_loss = model.predicted_loss(x, y, w_pred).item()\n",
    "                    losses[\"prediction\"].append(predicted_loss)\n",
    "                    loss_audit = model.loss_audit(x, s, f, w_audit).item()\n",
    "                    losses[\"audit\"].append(loss_audit)\n",
    "                    loss_total = model.loss(x, y, s, f, w_pred, w_audit).item()\n",
    "                    losses[\"total\"].append(loss_total)\n",
    "\n",
    "            if epoch_i % 25 == 24:\n",
    "                print(\n",
    "                    \"Epoch {:>3} / {}: prediction loss {:.6f}, fairness loss {:.6f}\".format(\n",
    "                        epoch_i + 1,\n",
    "                        self.epoch,\n",
    "                        np.sum(losses[\"prediction\"]),\n",
    "                        np.sum(losses[\"audit\"]),\n",
    "                    )\n",
    "                )\n",
    "            if epoch_i % 100 == 99:\n",
    "                self.save(epoch_i + 1)\n",
    "\n",
    "            if self.tensorboard:\n",
    "                summary_writer.add_scalar(\n",
    "                    \"train/prediction loss\", np.sum(losses[\"prediction\"]), epoch_i\n",
    "                )\n",
    "                summary_writer.add_scalar(\n",
    "                    \"train/fairness loss\", np.sum(losses[\"audit\"]), epoch_i\n",
    "                )\n",
    "                summary_writer.add_scalar(\n",
    "                    \"train/total loss\", np.sum(losses[\"total\"]), epoch_i\n",
    "                )\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    df = self.dataset.val\n",
    "                    x_idx = df.columns.values.tolist()\n",
    "                    x_idx.remove(\"result\")\n",
    "                    x = torch.from_numpy(df[x_idx].values).type(torch.float).to(device)\n",
    "                    y_pred = model.forward_y(x).view(-1).cpu().numpy()\n",
    "\n",
    "                    res = self.dataset.analyze(df, y_pred, log=False)\n",
    "\n",
    "                    acc = res[\"acc\"]\n",
    "                    DP = res[\"DP\"]\n",
    "                    EO = res[\"EO\"]\n",
    "                    CF = res[\"CF\"]\n",
    "\n",
    "                    if self.tensorboard:\n",
    "                        summary_writer.add_scalar(\n",
    "                            \"train/validation accuracy\", acc, epoch_i\n",
    "                        )\n",
    "                        summary_writer.add_scalar(\"train/validation DP\", DP, epoch_i)\n",
    "                        summary_writer.add_scalar(\"train/validation EO\", EO, epoch_i)\n",
    "                        summary_writer.add_scalar(\"train/validation CF\", CF, epoch_i)\n",
    "\n",
    "        if self.tensorboard:\n",
    "            summary_writer.close()\n",
    "        self.save(\"last\")\n",
    "        with open((self.res_dir + \"/\" + \"train_done.txt\"), \"w\") as f:\n",
    "            f.write(\"done\")\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(self, version=\"last\"):\n",
    "        if not os.path.exists(self.model_dir + \"/\" + f\"{version}.pth\"):\n",
    "            print(\"Model has not been trained!!\")\n",
    "            return\n",
    "\n",
    "        name = self.output_name\n",
    "        print(\n",
    "            \"=============================================================================\"\n",
    "        )\n",
    "        print(f\"Finetuning for {name}\")\n",
    "        print(\n",
    "            \"=============================================================================\"\n",
    "        )\n",
    "\n",
    "        if os.path.exists(self.res_dir + \"/\" + f\"finetune_{version}_done.txt\"):\n",
    "            print(\"Model has been finetuned!!\")\n",
    "            return\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.load(version)\n",
    "        model = self.model.to(device)\n",
    "        model.finetune_only()\n",
    "        optimizer = getattr(optim, self.optim)(model.finetune_params(), lr=self.lr)\n",
    "\n",
    "        if self.tensorboard:\n",
    "            summary_writer = SummaryWriter(self.tensorboard_dir)\n",
    "        max_acc = 0\n",
    "        max_epoch = 0\n",
    "\n",
    "        for epoch_i in range(self.epoch):\n",
    "            losses = []\n",
    "            for step, batch in enumerate(self.train_dataloader):\n",
    "                x = batch[0].to(device)\n",
    "                y = batch[1].to(device)\n",
    "                _ = batch[2].to(device)\n",
    "                f = batch[3].to(device)\n",
    "                w_pred = batch[4].to(device)\n",
    "                _ = batch[5].to(device)\n",
    "\n",
    "                loss = model.predicted_lossloss_prediction(x, y, w_pred)\n",
    "                losses.append(loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if epoch_i % 25 == 24:\n",
    "                print(\n",
    "                    \"Epoch {:>3} / {}: prediction loss {:.6f}\".format(\n",
    "                        epoch_i + 1,\n",
    "                        self.epoch,\n",
    "                        np.sum(losses),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if self.tensorboard:\n",
    "                summary_writer.add_scalar(\n",
    "                    f\"finetune_{version}/training loss\", np.sum(losses), epoch_i\n",
    "                )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                df = self.dataset.val\n",
    "                x_idx = df.columns.values.tolist()\n",
    "                x_idx.remove(\"result\")\n",
    "                x = torch.from_numpy(df[x_idx].values).type(torch.float).to(device)\n",
    "                y_pred = model.forward_y(x).view(-1).cpu().numpy()\n",
    "\n",
    "                res = self.dataset.analyze(df, y_pred, log=False)\n",
    "\n",
    "                acc = res[\"acc\"]\n",
    "                DP = res[\"DP\"]\n",
    "                EO = res[\"EO\"]\n",
    "                CF = res[\"CF\"]\n",
    "\n",
    "                if self.tensorboard:\n",
    "                    summary_writer.add_scalar(\n",
    "                        f\"finetune_{version}/validation accuracy\", acc, epoch_i\n",
    "                    )\n",
    "                    summary_writer.add_scalar(\n",
    "                        f\"finetune_{version}/validation DP\", DP, epoch_i\n",
    "                    )\n",
    "                    summary_writer.add_scalar(\n",
    "                        f\"finetune_{version}/validation EO\", EO, epoch_i\n",
    "                    )\n",
    "                    summary_writer.add_scalar(\n",
    "                        f\"finetune_{version}/validation CF\", CF, epoch_i\n",
    "                    )\n",
    "\n",
    "                if acc > max_acc:\n",
    "                    max_epoch = epoch_i\n",
    "                    max_acc = acc\n",
    "                    self.save(f\"finetune_{version}_best\")\n",
    "\n",
    "            if epoch_i - max_epoch > 20:\n",
    "                print(\" Early stop!\")\n",
    "                break\n",
    "\n",
    "        if self.tensorboard:\n",
    "            summary_writer.close()\n",
    "        self.save(f\"finetune_{version}_last\")\n",
    "        with open((self.res_dir + \"/\" + f\"finetune_{version}_done.txt\"), \"w\") as f:\n",
    "            f.write(\"done\")\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self, version=\"last\", log=False):\n",
    "    name = self.output_name\n",
    "    if not os.path.exists(self.model_dir + \"/\" + f\"{version}.pth\"):\n",
    "        return None\n",
    "\n",
    "    print(\n",
    "        \"=============================================================================\"\n",
    "    )\n",
    "    print(f\"Testing for {name}\")\n",
    "    print(\n",
    "        \"=============================================================================\"\n",
    "    )\n",
    "\n",
    "    res_name = self.res_dir + \"/\" f\"test_{version}.json\"\n",
    "\n",
    "    if not os.path.exists(res_name):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.load(version)\n",
    "        model = self.model.to(device)\n",
    "        df = self.dataset.test\n",
    "        x_idx = df.columns.values.tolist()\n",
    "        x_idx.remove(\"result\")\n",
    "        x = torch.from_numpy(df[x_idx].values).type(torch.float).to(device)\n",
    "        f = (\n",
    "            torch.from_numpy(df[self.dataset.fair_variables].values)\n",
    "            .type(torch.float)\n",
    "            .to(device)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            y_pred = model.forward_y(x).view(-1).cpu().numpy()\n",
    "        res = dict()\n",
    "        res[\"test\"] = self.dataset.analyze(self.dataset.test, y_pred, log=log)\n",
    "        with open(res_name, \"w\") as f:\n",
    "            f.write(json.dumps(res, indent=4))\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(res_name, \"r\") as f:\n",
    "            res = json.loads(f.read())\n",
    "    for key, value in res[\"test\"].items():\n",
    "        print(key, \"=\", value)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def main():\n",
    "    config_file = open(\"./results/config.json\")\n",
    "    config = json.load(config_file)\n",
    "    dataset = config[\"dataset\"]\n",
    "    \n",
    "    torch.random.manual_seed(config[\"seed\"])\n",
    "    np.random.seed(config[\"seed\"])\n",
    "    \n",
    "    if config[\"model\"] == \"DCFR\":\n",
    "        if config[\"task\"] == \"DP\":\n",
    "            fair_variables =[]\n",
    "        elif config[\"task\"] == \"EO\":\n",
    "            fair_variables = [\"result\"]\n",
    "        elif config[\"task\"] == \"CF\":\n",
    "            pass\n",
    "        model = Model(config, fair_variables)\n",
    "        \n",
    "    runner = Data_Trainer(dataset, model, config)\n",
    "    \n",
    "    runner.train()\n",
    "    runner.finetune(\"last\")\n",
    "    runner.test(\"finetune_last_best\")\n",
    "    \n",
    "    fairness = model['Fairness']\n",
    "    DP = model['DP']\n",
    "    CF = model['CF']\n",
    "    EO = model['EO']\n",
    "\n",
    "    plt.plot(fairness, DP, 'b', label= 'DP')\n",
    "    plt.plot(fairness, CF, 'r', label=\"CF\")\n",
    "    plt.plot(fairness, EO, 'm', label=\"EO\")\n",
    "    plt.title('Adult / $\\Delta$ DP, $\\Delta$ CF, $\\Delta$ EO')\n",
    "    plt.xlabel('Fainess')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./results/result.jpeg\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
